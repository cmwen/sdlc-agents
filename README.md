# Agentic SDLC with AI-in-the-Loop

This repository defines a lightweight, role-based Agentic SDLC where multiple specialized agents collaborate to deliver software with strong traceability and human oversight. Each agent is a prompt file in `.github/prompts/` and is designed to be used with AI coding tools (for example, GitHub Copilot, Claude Code, Cursor) as a working companionâ€”not a single monolithic assistant.

## ðŸš€ Quick Start

Install the CLI tool globally:

```bash
npm install -g @cmwen/sdlc-agents
```

Or use with npx (no installation required):

```bash
npx @cmwen/sdlc-agents init
```

### Commands

- `sdlc-agents init` - Initialize a new project with prompts and documentation structure
- `sdlc-agents install` - Install only the prompt files
- `sdlc-agents list` - List all available agents
- `sdlc-agents --help` - Show help information

### Options

- `-p, --path <path>` - Installation path (default: `.github/prompts`)
- `-f, --force` - Overwrite existing files

## The Agents

The agents in this repo:
- Research â€” `.github/prompts/research.prompt.md`
- Vision â€” `.github/prompts/vision.prompt.md`
- Product â€” `.github/prompts/product.prompt.md`
- Design â€” `.github/prompts/design.prompt.md`
- Execution â€” `.github/prompts/execution.prompt.md`
- QA â€” `.github/prompts/qa.prompt.md`
- Governance â€” `.github/prompts/governance.prompt.md`

All prompts encourage agents to use tools to gather external context when helpful.

---

## Core principles

1. Human-in-the-loop: People review and approve key steps. Agents challenge assumptions and surface risks.
2. Role specialization: Each agent focuses on its domain (Vision, Product, Design, Execution, QA, Governance) and hands off clearly to the next.
3. Traceability-by-default: Every output links backward and forward across the lifecycle using Markdown docs and labels.
4. Tools for context: Agents are encouraged to use tools (for example, MCP tools or IDE-integrated actions) to research and validate decisions.

---

## How the agents collaborate

Simple ASCII view of the forward flow and feedback loops:

```
                              [Research] (Knowledge & Insights)
                                   |
                                   v
                        /docs/research/index.md
                        /docs/research/topics/*.md
                        /docs/research/conversations/*.md
                                   |
     Research feeds into all stages and receives requests from any stage
                                   |
                                   v
 [Vision] --[Vision â†’ Product]--> [Product] --[Product â†’ Design]--> [Design]
       |                                   |                             |
       v                                   v                             v
   /docs/vision.md                  /docs/product_backlog.md       /docs/design.md

 [Design] --[Design â†’ Execution]--> [Execution] --[Execution â†’ QA]--> [QA]
       |                                   |                             |
       v                                   v                             v
 (design decisions)                /docs/execution_log.md         /docs/qa_plan.md

 Feedback loops:
    [QA] --[QA â†’ Execution: Bug]--> [Execution]
    [QA] --[QA â†’ Design: Flaw]----> [Design]
    [Any Stage] --[Stage â†’ Research]--> [Research] --[Research â†’ Stage]--> [Any Stage]

 Governance (process guardian & traceability):
    [Governance] watches all stages, flags missing links, and maintains
    /docs/governance_traceability.md; provides in-flight summaries and readiness checks.
```

Notes:
- Labels in brackets are used to tag handoffs and create an audit trail in Markdown.
- Governance coordinates handoffs and guards traceability; it is not a methodology label.

---

## Traceability map (Markdown docs)

- `/docs/research/index.md` â€” Master index of research activities, topics, and knowledge. Cross-references to all SDLC stages. [Research â†” All Stages]
- `/docs/research/topics/*.md` â€” Deep-dive research on specific topics with multi-perspective analysis. [Research â†’ Relevant Stages]
- `/docs/research/conversations/*.md` â€” Brainstorming sessions, learning activities, and research discussions. [Research â†” All Stages]
- `/docs/vision.md` â€” Problem statement, user scenarios, success criteria, risks. [Vision â†’ Product]
- `/docs/product_backlog.md` â€” Epics, features, acceptance criteria; linked to Vision and QA. [Product â†’ Design]
- `/docs/design.md` â€” Architecture, sequence flows, data models, trade-offs; linked to backlog items and Execution notes. [Design â†’ Execution]
- `/docs/execution_log.md` â€” Implemented features, linked design decisions and backlog items, suggested tests. [Execution â†’ QA]
- `/docs/qa_plan.md` â€” Test scenarios mapped to acceptance criteria and design decisions; regression risks tied to execution notes. [QA â†’ Governance]
- `/docs/governance_traceability.md` â€” Cross-references, gaps, and audit notes across all artifacts.

---

## Quickstart workflow

1. **Research** (optional but recommended): For complex or unfamiliar domains, start with research to gather context, explore approaches, and identify best practices.
2. Start with Vision: clarify scope and write `/docs/vision.md`.
3. Product creates `/docs/product_backlog.md` with acceptance criteria.
4. Design proposes options and documents `/docs/design.md` with trade-offs and risks.
5. Execution implements changes, updates `/docs/execution_log.md`, and tags handoff to QA.
6. QA drafts `/docs/qa_plan.md`, files bugs using labels (for example, `[QA â†’ Execution: Bug]`).
7. Governance enforces links, highlights gaps, and prepares readiness checks.

**Research Integration**: Any agent can request research support using `[Stage â†’ Research]` labels, and Research provides insights back using `[Research â†’ Stage]` labels.

---

## Best practices

- Keep prompts brief but explicit; link to the relevant `/docs/*.md` artifacts in your request.
- Use the labels defined in the prompts to tag handoffs and feedback loops.
- Prefer adding context (files, diffs) to chats over long proseâ€”let the agent read the source.
- Ask agents to compare at least two approaches when thereâ€™s ambiguity and to record trade-offs in `/docs/design.md`.
- Encourage the agent to use tools for external validation (benchmarks, API docs, security checks) and to capture references in the docs.

---

## License

MIT
